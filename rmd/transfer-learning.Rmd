

>神经网络需要用数据来训练，它从数据中获得信息，进而把它们转换成相应的权重。这些权重能够被提取出来，迁移到其他的神经网络中，我们“迁移”了这些学来的特征，就不需要从零开始训练一个神经网络了。**把已学训练好的模型参数迁移到新的模型来帮助新模型训练**。考虑到大部分数据或任务是存在相关性的，所以通过迁移学习我们可以将已经学到的模型参数（也可理解为模型学到的知识）通过某种方式来分享给新模型从而加快并优化模型的学习效率不用像大多数网络那样从零学习（starting from scratch，tabula rasa）。[@迁移学习]

>迁移学习允许我们通过借用已经存在的一些相关的任务或域的标签数据来处理这些场景。我们尝试着把在源域中解决源任务时获得的知识存储下来，并将其应用在我们感兴趣的目标域中的目标任务上去.

>其核心思想在于将一些训练集充足的任务（如ImageNet、Places365分类等）引入到当前任务中联合进行训练，其中引入的辅助任务并不使用其原有的全部训练集，而是挑选出那些底层特征和当前主要任务训练集相似的那部分图像。这样利用可直接获取的额外数据来辅助主要任务的训练，可使得模型获得较大的精度提升。[Momenta](https://www.zhihu.com/question/41979241/answer/465698997)


简单的来说就是不需要重新全部再学习一遍了，而是在原有的基础上增加缺少的部分就好了，站在巨人的肩膀上看世界。

## 预训练模型
